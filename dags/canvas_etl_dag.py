#
# canvas_etl_dag.py â€” FINAL VERSION (multi-course + stable)
#
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import requests
import pandas as pd
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv

# --- LOAD ENVIRONMENT VARIABLES ---
load_dotenv()

CANVAS_API_BASE_URL = os.getenv("CANVAS_API_BASE_URL")
CANVAS_API_TOKEN = os.getenv("CANVAS_API_TOKEN")
COURSE_IDS = os.getenv("COURSE_IDS").split(",")
DB_CONNECTION_STRING = os.getenv("DB_CONNECTION_STRING")

HEADERS = {"Authorization": f"Bearer {CANVAS_API_TOKEN}"}


# --- TASK 1: EXTRACT COURSE INFO ---
def extract_courses():
    """Láº¥y thÃ´ng tin tÃªn khÃ³a há»c tá»« Canvas API vÃ  lÆ°u vÃ o dim_courses"""
    courses_data = []

    for cid in COURSE_IDS:
        res = requests.get(f"{CANVAS_API_BASE_URL}courses/{cid}", headers=HEADERS)
        if res.status_code == 200:
            data = res.json()
            course_name = data.get("name", f"Course {cid}")
            courses_data.append({"course_id": str(cid), "course_name": course_name})
            print(f"âœ… {cid} - {course_name}")
        else:
            print(f"âš ï¸ KhÃ´ng láº¥y Ä‘Æ°á»£c thÃ´ng tin cho course {cid}")

    df_courses = pd.DataFrame(courses_data)
    engine = create_engine(DB_CONNECTION_STRING)

    # ğŸ§© DÃ¹ng 'replace' chá»‰ 1 láº§n Ä‘á»ƒ cáº­p nháº­t láº¡i toÃ n bá»™ danh sÃ¡ch khÃ³a há»c
    try:
        df_courses.to_sql("dim_courses", engine, if_exists="replace", index=False)
        print(f"ğŸ“š ÄÃ£ lÆ°u {len(df_courses)} khÃ³a há»c vÃ o dim_courses.")
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi táº£i dim_courses: {e}")

def extract_submissions_data():
    all_submissions = []

    for course_id in COURSE_IDS:
        print(f"\nğŸ“ Äang xá»­ lÃ½ khÃ³a há»c: {course_id}")
        url = f"{CANVAS_API_BASE_URL}courses/{course_id}/assignments"
        res = requests.get(url, headers=HEADERS)

        if res.status_code != 200:
            print(f"âš ï¸ KhÃ´ng thá»ƒ truy cáº­p khÃ³a há»c {course_id} â€” status {res.status_code}")
            continue

        assignments = res.json()
        if not isinstance(assignments, list) or len(assignments) == 0:
            print(f"âš ï¸ KhÃ³a há»c {course_id} khÃ´ng cÃ³ assignments.")
            continue

        print(f"ğŸ“š {len(assignments)} bÃ i táº­p tÃ¬m tháº¥y trong khÃ³a {course_id}")

        for a in assignments:
            assignment_id = a["id"]
            submissions_url = (
                f"{CANVAS_API_BASE_URL}courses/{course_id}/assignments/{assignment_id}/submissions?include[]=user"
            )
            subs = requests.get(submissions_url, headers=HEADERS).json()
            for s in subs:
                s["course_id"] = str(course_id)
            all_submissions.extend(subs)
            print(f"âœ… {len(subs)} submissions tá»« {a['name']} (course {course_id})")

    if len(all_submissions) == 0:
        print("âš ï¸ KhÃ´ng cÃ³ submissions nÃ o Ä‘Æ°á»£c láº¥y â€” kiá»ƒm tra láº¡i cÃ¡c course ID hoáº·c assignments trÃªn Canvas.")
        return

    df = pd.json_normalize(all_submissions)
    df.to_csv("/tmp/raw_submissions.csv", index=False)
    print(f"ğŸ“¦ ÄÃ£ trÃ­ch xuáº¥t {len(df)} submissions tá»« {len(COURSE_IDS)} khÃ³a há»c.")



# --- TASK 3: TRANSFORM & LOAD ---
def transform_and_load_data():
    """LÃ m sáº¡ch vÃ  táº£i dá»¯ liá»‡u submissions vÃ o Data Warehouse"""
    df = pd.read_csv("/tmp/raw_submissions.csv")

    # ğŸ§  Äáº£m báº£o cÃ³ cá»™t course_id
    if "course_id" not in df.columns:
        print("âš ï¸ Thiáº¿u cá»™t course_id trong dá»¯ liá»‡u! Kiá»ƒm tra láº¡i hÃ m extract_submissions_data.")
        return

    # --- DIM STUDENTS ---
    df_students = df[["user.id", "user.name", "user.login_id"]].drop_duplicates()
    df_students.columns = ["student_id", "student_name", "student_email"]

    # --- FACT SUBMISSIONS ---
    df_facts = df[["id", "user.id", "assignment_id", "submitted_at", "grade", "late", "course_id"]].rename(
        columns={"id": "submission_id", "user.id": "student_id"}
    )
    df_facts["submitted_at"] = pd.to_datetime(df_facts["submitted_at"], errors="coerce")
    df_facts["grade"] = pd.to_numeric(df_facts["grade"], errors="coerce")
    df_facts["course_id"] = df_facts["course_id"].astype(str)

    # --- LOAD TO DATABASE ---
    engine = create_engine(DB_CONNECTION_STRING)
    with engine.connect() as conn:
        try:
            # Ghi thÃªm sinh viÃªn (append)
            df_students.to_sql("dim_students", conn, if_exists="append", index=False)
            print(f"ğŸ‘©â€ğŸ“ ÄÃ£ táº£i {len(df_students)} báº£n ghi vÃ o dim_students.")
        except Exception as e:
            print(f"âš ï¸ Lá»—i táº£i dim_students (cÃ³ thá»ƒ trÃ¹ng khÃ³a): {e}")

        # Ghi thÃªm submissions
        df_facts.to_sql("fact_submissions", conn, if_exists="append", index=False)
        print(f"ğŸ“Š ÄÃ£ táº£i {len(df_facts)} báº£n ghi vÃ o fact_submissions.")


# --- DAG DEFINITION ---
with DAG(
    dag_id="canvas_etl_pipeline",
    start_date=datetime(2025, 10, 10),
    schedule="@daily",
    catchup=False,
    tags=["canvas", "etl"],
) as dag:

    extract_courses_task = PythonOperator(
        task_id="extract_courses_info",
        python_callable=extract_courses,
    )

    extract_submissions_task = PythonOperator(
        task_id="extract_canvas_submissions",
        python_callable=extract_submissions_data,
    )

    transform_load_task = PythonOperator(
        task_id="transform_and_load_to_dwh",
        python_callable=transform_and_load_data,
    )

    extract_courses_task >> extract_submissions_task >> transform_load_task
