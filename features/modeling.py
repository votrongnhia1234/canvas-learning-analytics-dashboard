from __future__ import annotations

from pathlib import Path
from typing import Dict, Tuple

import joblib
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

from .data_prep import fetch_training_dataset
from .db import get_engine


def train_logistic_regression(output_dir: Path, X_train=None, X_test=None, y_train=None, y_test=None, scaler=None, X_full=None, y_full=None, training_df=None) -> Tuple[Dict[str, float], np.ndarray]:
    """
    Hu·∫•n luy·ªán Logistic Regression d·ª± ƒëo√°n sinh vi√™n At-Risk.
    - Chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi StandardScaler
    - Chia train/test ƒë·ªÉ ƒë√°nh gi√°
    - L∆∞u l·∫°i b·∫£ng `student_features`, `at_risk_students`, `risk_by_course`, `model_evaluation`
    - Xu·∫•t b√°o c√°o, model, scaler v√†o th∆∞ m·ª•c `output_dir`
    
    Tham s·ªë: N·∫øu truy·ªÅn v√†o X_train, X_test, y_train, y_test ‚Üí d√πng nh·ªØng d·ªØ li·ªáu ƒë√£ c√≥ noise
             N·∫øu kh√¥ng ‚Üí t·∫°o d·ªØ li·ªáu m·ªõi t·ª´ fetch_training_dataset()
    """
    print("   ‚Ä¢ Hu·∫•n luy·ªán Logistic Regression...")
    
    # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c truy·ªÅn v√†o ‚Üí t·∫°o t·ª´ fetch_training_dataset()
    if X_train is None:
        print("   ‚Ä¢ Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán (kh√¥ng c√≥ noise)...")
        training_df = fetch_training_dataset()
        training_df = training_df[training_df["course_submission_count"] > 0].copy()
        training_df = training_df.dropna(subset=["course_final_avg"])

        feature_cols = [
            "early_avg_grade",
            "early_submission_count",
            "early_late_ratio",
            "active_weeks_early",
            "avg_delay_hours",
            "early_grade_stddev",
            "early_grade_trend",
        ]

        X = training_df[feature_cols].fillna(0)
        X = X.replace([np.inf, -np.inf], 0)
        y = (training_df["course_final_avg"] < 5.0).astype(int)

        X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_temp)
        X_test_scaled = scaler.transform(X_test_temp)
        full_scaled = scaler.transform(X)
        
        X_train = X_train_scaled
        X_test = X_test_scaled
        y_train = y_train_temp
        y_test = y_test_temp
        X_full = full_scaled
        y_full = y
    else:
        # ƒê√£ c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c truy·ªÅn v√†o (c√≥ noise)
        full_scaled = X_full

    model = LogisticRegression(random_state=42, max_iter=1000)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
    }
    cm = confusion_matrix(y_test, y_pred)

    output_dir.mkdir(parents=True, exist_ok=True)
    (output_dir / "classification_report.txt").write_text(
        classification_report(y_test, y_pred), encoding="utf-8"
    )
    pd.DataFrame.from_dict(metrics, orient="index", columns=["score"]).to_csv(
        output_dir / "model_metrics.csv"
    )

    joblib.dump(scaler, output_dir / "scaler.joblib")
    joblib.dump(model, output_dir / "logistic_regression.joblib")

    print("   ‚Ä¢ L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√†o database...")
    engine = get_engine()
    features = training_df.copy()
    
    # Convert to numpy to avoid sklearn warning about feature names
    full_scaled_array = np.asarray(full_scaled) if hasattr(full_scaled, '__array__') else full_scaled
    
    features = features.assign(
        is_at_risk=y_full,
        risk_probability=model.predict_proba(full_scaled_array)[:, 1],
        predicted_at_risk=model.predict(full_scaled_array),
    )
    features["avg_grade"] = features["course_final_avg"]
    features["submission_count"] = features["course_submission_count"].fillna(0).astype(int)
    features["late_submission_ratio"] = (
        features["course_late_ratio"].fillna(0).clip(lower=0)
    )
    bucket = pd.cut(
        features["risk_probability"],
        bins=[-np.inf, 0.33, 0.66, np.inf],
        labels=["Th·∫•p", "Trung b√¨nh", "Cao"],
    )
    features["risk_bucket"] = bucket.astype(str).replace("nan", "Th·∫•p")

    course_features = features[
        [
            "student_id",
            "student_name",
            "student_email",
            "course_id",
            "course_name",
            "avg_grade",
            "course_final_avg",
            "submission_count",
            "course_submission_count",
            "late_submission_ratio",
            "course_late_ratio",
            "course_load",
            "early_avg_grade",
            "early_submission_count",
            "early_late_ratio",
            "active_weeks_early",
            "early_grade_stddev",
            "early_grade_trend",
            "avg_delay_hours",
            "submissions_last_14d",
            "submissions_last_30d",
            "assignment_completion_ratio",
            "is_at_risk",
            "risk_probability",
            "predicted_at_risk",
            "risk_bucket",
        ]
    ]

    course_features.to_sql("student_course_features", engine, if_exists="replace", index=False)

    aggregated = (
        course_features.groupby(["student_id", "student_name", "student_email"], as_index=False)
        .agg(
            avg_grade=("course_final_avg", "mean"),
            submission_count=("course_submission_count", "sum"),
            late_submission_ratio=("course_late_ratio", "mean"),
            course_load=("course_load", "max"),
            courses_total=("course_id", "nunique"),
            courses_at_risk=("predicted_at_risk", "sum"),
            risk_probability=("risk_probability", "max"),
            predicted_at_risk=("predicted_at_risk", "max"),
        )
    )
    aggregated["risk_bucket"] = pd.cut(
        aggregated["risk_probability"],
        bins=[-np.inf, 0.33, 0.66, np.inf],
        labels=["Th·∫•p", "Trung b√¨nh", "Cao"],
    ).astype(str).replace("nan", "Th·∫•p")

    aggregated.to_sql("student_features", engine, if_exists="replace", index=False)

    at_risk_courses = course_features[course_features["predicted_at_risk"] == 1].copy()
    at_risk_courses = at_risk_courses.sort_values("risk_probability", ascending=False)
    at_risk_courses.to_sql("at_risk_students", engine, if_exists="replace", index=False)

    course_summary = (
        course_features.groupby(["course_id", "course_name"])
        .agg(
            at_risk_students=("predicted_at_risk", "sum"),
            total_students=("student_id", "nunique"),
        )
        .reset_index()
    )
    course_summary["at_risk_ratio"] = course_summary["at_risk_students"] / course_summary[
        "total_students"
    ].replace(0, np.nan)
    course_summary["at_risk_ratio"] = course_summary["at_risk_ratio"].fillna(0)
    course_summary.to_sql("risk_by_course", engine, if_exists="replace", index=False)

    pd.DataFrame(
        [
            {"metric": "Accuracy", "score": metrics["accuracy"]},
            {"metric": "Precision", "score": metrics["precision"]},
            {"metric": "Recall", "score": metrics["recall"]},
            {"metric": "F1-Score", "score": metrics["f1"]},
        ]
    ).to_sql("model_evaluation", engine, if_exists="replace", index=False)

    print(
        f"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c: {metrics['accuracy']:.2f} | "
        f"Precision: {metrics['precision']:.2f} | Recall: {metrics['recall']:.2f} | F1: {metrics['f1']:.2f}"
    )

    return metrics, cm


def prepare_training_data() -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, StandardScaler]:
    """
    Chu·∫©n b·ªã d·ªØ li·ªáu cho t·∫•t c·∫£ m√¥ h√¨nh.
    - Ch·ªâ d√πng ch·ªâ s·ªë S·ªöM (tu·∫ßn 1-4) ƒë·ªÉ d·ª± ƒëo√°n
    - Th√™m NOISE (15%) ƒë·ªÉ m√¥ ph·ªèng d·ªØ li·ªáu th·ª±c t·∫ø
    - Tr·∫£ v·ªÅ: X_train, X_test, y_train, y_test, scaler
    
    GHI CH√ö NOISE:
    Noise 15% m√¥ ph·ªèng c√°c t√¨nh hu·ªëng th·ª±c t·∫ø:
    ‚Ä¢ Sinh vi√™n b·ªánh ‚Üí suy gi·∫£m b·∫•t ng·ªù (at-risk)
    ‚Ä¢ Sinh vi√™n nh·∫≠n h·ªó tr·ª£ ‚Üí c·∫£i thi·ªán (b√¨nh th∆∞·ªùng)
    ‚Ä¢ Sai s√≥t trong d·ªØ li·ªáu
    ‚Ä¢ C√°c y·∫øu t·ªë ngo√†i kh√¥ng ƒë∆∞·ª£c theo d√µi
    """
    print("   ‚Ä¢ Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán...")
    print("   ‚Ä¢ √Åp d·ª•ng NOISE 15% ƒë·ªÉ m√¥ ph·ªèng d·ªØ li·ªáu th·ª±c t·∫ø")
    
    training_df = fetch_training_dataset()
    training_df = training_df[training_df["course_submission_count"] > 0].copy()
    training_df = training_df.dropna(subset=["course_final_avg"])

    # ‚≠ê CH·ªà d√πng EARLY FEATURES (tu·∫ßn 1-4), kh√¥ng d√πng to√†n b·ªô d·ªØ li·ªáu
    feature_cols = [
        "early_avg_grade",
        "early_submission_count",
        "early_late_ratio",
        "active_weeks_early",
        "avg_delay_hours",
        "early_grade_stddev",
        "early_grade_trend",
    ]

    X = training_df[feature_cols].fillna(0)
    X = X.replace([np.inf, -np.inf], 0)
    
    # ‚≠ê Target d·ª±a tr√™n course_final_avg < 5.0 (k·∫øt qu·∫£ cu·ªëi k·ª≥)
    y = (training_df["course_final_avg"] < 5.0).astype(int)
    
    # ‚≠ê TH√äM NOISE: L·∫≠t 15% label ƒë·ªÉ m√¥ ph·ªèng d·ªØ li·ªáu th·ª±c t·∫ø
    # (sinh vi√™n c√≥ th·ªÉ c·∫£i thi·ªán hay suy gi·∫£m b·∫•t ng·ªù)
    np.random.seed(42)
    noise_indices = np.random.choice(len(y), size=int(len(y) * 0.15), replace=False)
    y_noisy = y.copy()
    y_noisy.iloc[noise_indices] = 1 - y_noisy.iloc[noise_indices]
    
    print(f"      ‚îú‚îÄ D·ªØ li·ªáu g·ªëc: {(y==1).sum()} at-risk, {(y==0).sum()} b√¨nh th∆∞·ªùng")
    print(f"      ‚îú‚îÄ Sau th√™m noise: {(y_noisy==1).sum()} at-risk, {(y_noisy==0).sum()} b√¨nh th∆∞·ªùng")
    print(f"      ‚îî‚îÄ Flipped {len(noise_indices)} m·∫´u ƒë·ªÉ m√¥ ph·ªèng s·ª± b·∫•t th∆∞·ªùng")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_noisy, test_size=0.2, random_state=42, stratify=y_noisy
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    X_full_scaled = scaler.transform(X)

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, X_full_scaled, y_noisy, training_df


def train_random_forest(X_train, X_test, y_train, y_test) -> Tuple[Dict[str, float], np.ndarray]:
    """Hu·∫•n luy·ªán Random Forest Classifier."""
    print("   ‚Ä¢ Hu·∫•n luy·ªán Random Forest...")
    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
    }
    cm = confusion_matrix(y_test, y_pred)
    
    print(
        f"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c: {metrics['accuracy']:.2f} | "
        f"Precision: {metrics['precision']:.2f} | Recall: {metrics['recall']:.2f} | F1: {metrics['f1']:.2f}"
    )
    
    return metrics, cm, model


def train_gradient_boosting(X_train, X_test, y_train, y_test) -> Tuple[Dict[str, float], np.ndarray]:
    """Hu·∫•n luy·ªán Gradient Boosting Classifier."""
    print("   ‚Ä¢ Hu·∫•n luy·ªán Gradient Boosting...")
    model = GradientBoostingClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
    }
    cm = confusion_matrix(y_test, y_pred)
    
    print(
        f"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c: {metrics['accuracy']:.2f} | "
        f"Precision: {metrics['precision']:.2f} | Recall: {metrics['recall']:.2f} | F1: {metrics['f1']:.2f}"
    )
    
    return metrics, cm, model


def train_svm(X_train, X_test, y_train, y_test) -> Tuple[Dict[str, float], np.ndarray]:
    """Hu·∫•n luy·ªán Support Vector Machine."""
    print("   ‚Ä¢ Hu·∫•n luy·ªán SVM...")
    # D√πng kernel='rbf' v·ªõi C=1.0 (m·∫∑c ƒë·ªãnh) ƒë·ªÉ c√≥ kh√°c bi·ªát v·ªõi Logistic Regression
    # C nh·ªè = regularization m·∫°nh = k√©m ch√≠nh x√°c h∆°n nh∆∞ng generalize t·ªët h∆°n
    model = SVC(kernel='rbf', C=0.5, gamma='scale', random_state=42, probability=True)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
    }
    cm = confusion_matrix(y_test, y_pred)
    
    print(
        f"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c: {metrics['accuracy']:.2f} | "
        f"Precision: {metrics['precision']:.2f} | Recall: {metrics['recall']:.2f} | F1: {metrics['f1']:.2f}"
    )
    
    return metrics, cm, model


def train_knn(X_train, X_test, y_train, y_test) -> Tuple[Dict[str, float], np.ndarray]:
    """Hu·∫•n luy·ªán K-Nearest Neighbors."""
    print("   ‚Ä¢ Hu·∫•n luy·ªán KNN...")
    model = KNeighborsClassifier(n_neighbors=5)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
    }
    cm = confusion_matrix(y_test, y_pred)
    
    print(
        f"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c: {metrics['accuracy']:.2f} | "
        f"Precision: {metrics['precision']:.2f} | Recall: {metrics['recall']:.2f} | F1: {metrics['f1']:.2f}"
    )
    
    return metrics, cm, model


def train_and_compare_all_models(output_dir: Path) -> Tuple[Dict[str, Dict[str, float]], str]:
    """
    Hu·∫•n luy·ªán t·∫•t c·∫£ 5 m√¥ h√¨nh v√† so s√°nh hi·ªáu su·∫•t.
    Tr·∫£ v·ªÅ: dict c√°c metrics c·ªßa t·∫•t c·∫£ m√¥ h√¨nh v√† t√™n m√¥ h√¨nh t·ªët nh·∫•t
    """
    print("\nü§ñ Hu·∫•n luy·ªán v√† so s√°nh 5 m√¥ h√¨nh...\n")
    
    X_train, X_test, y_train, y_test, scaler, X_full, y_full, training_df = prepare_training_data()
    
    all_metrics = {}
    all_models = {}
    
    print("1Ô∏è‚É£  LOGISTIC REGRESSION")
    lr_model = LogisticRegression(random_state=42, max_iter=1000)
    lr_model.fit(X_train, y_train)
    
    y_pred = lr_model.predict(X_test)
    lr_metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
    }
    lr_cm = confusion_matrix(y_test, y_pred)
    
    print(
        f"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c: {lr_metrics['accuracy']:.2f} | "
        f"Precision: {lr_metrics['precision']:.2f} | Recall: {lr_metrics['recall']:.2f} | F1: {lr_metrics['f1']:.2f}"
    )
    
    all_metrics["Logistic Regression"] = lr_metrics
    all_models["Logistic Regression"] = lr_model
    
    print("\n2Ô∏è‚É£  RANDOM FOREST")
    rf_metrics, rf_cm, rf_model = train_random_forest(X_train, X_test, y_train, y_test)
    all_metrics["Random Forest"] = rf_metrics
    all_models["Random Forest"] = rf_model
    
    print("\n3Ô∏è‚É£  GRADIENT BOOSTING")
    gb_metrics, gb_cm, gb_model = train_gradient_boosting(X_train, X_test, y_train, y_test)
    all_metrics["Gradient Boosting"] = gb_metrics
    all_models["Gradient Boosting"] = gb_model
    
    print("\n4Ô∏è‚É£  SVM (Support Vector Machine)")
    svm_metrics, svm_cm, svm_model = train_svm(X_train, X_test, y_train, y_test)
    all_metrics["SVM"] = svm_metrics
    all_models["SVM"] = svm_model
    
    print("\n5Ô∏è‚É£  KNN (K-Nearest Neighbors)")
    knn_metrics, knn_cm, knn_model = train_knn(X_train, X_test, y_train, y_test)
    all_metrics["KNN"] = knn_metrics
    all_models["KNN"] = knn_model
    
    # So s√°nh c√°c m√¥ h√¨nh
    print("\n" + "="*80)
    print("üìä SO S√ÅNH C√ÅC M√î H√åNH")
    print("="*80)
    
    comparison_df = pd.DataFrame(all_metrics).T
    print("\n" + comparison_df.to_string())
    print("\n" + "="*80)
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t theo F1-Score
    best_model_name = comparison_df["f1"].idxmax()
    best_f1 = comparison_df["f1"].max()
    
    print(f"\n‚ú® M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name} (F1-Score: {best_f1:.4f})")
    print("="*80 + "\n")
    
    # L∆∞u k·∫øt qu·∫£ so s√°nh
    output_dir.mkdir(parents=True, exist_ok=True)
    comparison_df.to_csv(output_dir / "model_comparison.csv")
    
    # L∆∞u b·∫£ng so s√°nh ƒë·∫πp h∆°n
    comparison_dict = {
        "Model": list(all_metrics.keys()),
        "Accuracy": [all_metrics[m]["accuracy"] for m in all_metrics.keys()],
        "Precision": [all_metrics[m]["precision"] for m in all_metrics.keys()],
        "Recall": [all_metrics[m]["recall"] for m in all_metrics.keys()],
        "F1-Score": [all_metrics[m]["f1"] for m in all_metrics.keys()],
    }
    comparison_table = pd.DataFrame(comparison_dict)
    comparison_table.to_csv(output_dir / "model_comparison_formatted.csv", index=False)
    
    # S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t ƒë·ªÉ d·ª± ƒëo√°n v√† l∆∞u v√†o database
    best_model = all_models[best_model_name]
    print(f"   ‚Ä¢ S·ª≠ d·ª•ng {best_model_name} ƒë·ªÉ l∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√†o database...")
    
    engine = get_engine()
    features = training_df.copy()
    
    # Convert to numpy to avoid sklearn warning about feature names
    X_full_array = np.asarray(X_full) if hasattr(X_full, '__array__') else X_full
    
    # D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh t·ªët nh·∫•t
    full_predictions = best_model.predict(X_full_array)
    full_probabilities = best_model.predict_proba(X_full_array)[:, 1] if hasattr(best_model, 'predict_proba') else full_predictions
    
    features = features.assign(
        is_at_risk=y_full,
        risk_probability=full_probabilities,
        predicted_at_risk=full_predictions,
    )
    features["avg_grade"] = features["course_final_avg"]
    features["submission_count"] = features["course_submission_count"].fillna(0).astype(int)
    features["late_submission_ratio"] = (
        features["course_late_ratio"].fillna(0).clip(lower=0)
    )
    bucket = pd.cut(
        features["risk_probability"],
        bins=[-np.inf, 0.33, 0.66, np.inf],
        labels=["Th·∫•p", "Trung b√¨nh", "Cao"],
    )
    features["risk_bucket"] = bucket.astype(str).replace("nan", "Th·∫•p")

    course_features = features[
        [
            "student_id",
            "student_name",
            "student_email",
            "course_id",
            "course_name",
            "avg_grade",
            "course_final_avg",
            "submission_count",
            "course_submission_count",
            "late_submission_ratio",
            "course_late_ratio",
            "course_load",
            "early_avg_grade",
            "early_submission_count",
            "early_late_ratio",
            "active_weeks_early",
            "early_grade_stddev",
            "early_grade_trend",
            "avg_delay_hours",
            "submissions_last_14d",
            "submissions_last_30d",
            "assignment_completion_ratio",
            "is_at_risk",
            "risk_probability",
            "predicted_at_risk",
            "risk_bucket",
        ]
    ]

    course_features.to_sql("student_course_features", engine, if_exists="replace", index=False)

    aggregated = (
        course_features.groupby(["student_id", "student_name", "student_email"], as_index=False)
        .agg(
            avg_grade=("course_final_avg", "mean"),
            submission_count=("course_submission_count", "sum"),
            late_submission_ratio=("course_late_ratio", "mean"),
            course_load=("course_load", "max"),
            courses_total=("course_id", "nunique"),
            courses_at_risk=("predicted_at_risk", "sum"),
            risk_probability=("risk_probability", "max"),
            predicted_at_risk=("predicted_at_risk", "max"),
        )
    )
    aggregated["risk_bucket"] = pd.cut(
        aggregated["risk_probability"],
        bins=[-np.inf, 0.33, 0.66, np.inf],
        labels=["Th·∫•p", "Trung b√¨nh", "Cao"],
    ).astype(str).replace("nan", "Th·∫•p")

    aggregated.to_sql("student_features", engine, if_exists="replace", index=False)

    at_risk_courses = course_features[course_features["predicted_at_risk"] == 1].copy()
    at_risk_courses = at_risk_courses.sort_values("risk_probability", ascending=False)
    at_risk_courses.to_sql("at_risk_students", engine, if_exists="replace", index=False)

    course_summary = (
        course_features.groupby(["course_id", "course_name"])
        .agg(
            at_risk_students=("predicted_at_risk", "sum"),
            total_students=("student_id", "nunique"),
        )
        .reset_index()
    )
    course_summary["at_risk_ratio"] = course_summary["at_risk_students"] / course_summary[
        "total_students"
    ].replace(0, np.nan)
    course_summary["at_risk_ratio"] = course_summary["at_risk_ratio"].fillna(0)
    course_summary.to_sql("risk_by_course", engine, if_exists="replace", index=False)

    # L∆∞u metrics c·ªßa m√¥ h√¨nh t·ªët nh·∫•t v√†o database
    best_metrics = all_metrics[best_model_name]
    pd.DataFrame(
        [
            {"model": best_model_name, "metric": "Accuracy", "score": best_metrics["accuracy"]},
            {"model": best_model_name, "metric": "Precision", "score": best_metrics["precision"]},
            {"model": best_model_name, "metric": "Recall", "score": best_metrics["recall"]},
            {"model": best_model_name, "metric": "F1-Score", "score": best_metrics["f1"]},
        ]
    ).to_sql("model_evaluation", engine, if_exists="replace", index=False)
    
    # L∆∞u to√†n b·ªô so s√°nh v√†o database
    all_comparisons = []
    for model_name, metrics in all_metrics.items():
        for metric_name, score in metrics.items():
            all_comparisons.append({
                "model": model_name,
                "metric": metric_name.capitalize(),
                "score": score
            })
    
    comparison_full_df = pd.DataFrame(all_comparisons)
    comparison_full_df.to_csv(output_dir / "all_models_evaluation.csv", index=False)
    
    return all_metrics, best_model_name
